{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "m0jC9tz1zQsd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = load_diabetes(return_X_y=True)"
      ],
      "metadata": {
        "id": "RPvMg8Ps3Js4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF82lMX63XAP",
        "outputId": "7ef8529f-64b9-4708-ea9f-11afa669c76d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(442, 10)\n",
            "(442,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 2)"
      ],
      "metadata": {
        "id": "A_h-5T-J3cI9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient Descent (SGD) is an optimization algorithm widely used in machine learning to minimize loss functions, especially when working with large datasets. Unlike batch gradient descent, which updates model parameters after calculating the gradient across the entire dataset, SGD updates the parameters using only a single randomly selected data point at each step. This approach makes each update faster and allows the algorithm to handle massive datasets efficiently. However, the updates are noisier, resulting in a more erratic path toward the minimum, but this randomness can help the algorithm escape local minima and potentially find better solutions. SGD is particularly useful for online learning and real-time applications, where data arrives continuously and models need frequent updates."
      ],
      "metadata": {
        "id": "SNSAq5Xb4nWt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "a5AQ6CGShJ8J"
      },
      "outputs": [],
      "source": [
        "# Creating a custom class for Linear Regression using Stochastic Gradient Descent\n",
        "class SGDRegressor:\n",
        "\n",
        "    # Constructor to initialize learning rate and number of training epochs\n",
        "    def __init__(self, learning_rate=0.01, epochs=100):\n",
        "        self.coef_ = None        # This will hold the feature coefficients (weights)\n",
        "        self.intercept_ = None   # This will hold the bias/intercept\n",
        "        self.lr = learning_rate  # Learning rate for gradient descent\n",
        "        self.epochs = epochs     # Number of iterations for training\n",
        "\n",
        "    # Fit method: used to train the model on given data\n",
        "    def fit(self, X_train, y_train):\n",
        "        # Initialize the intercept to 0\n",
        "        self.intercept_ = 0\n",
        "\n",
        "        # Initialize coefficients to 1 for all features (same shape as number of columns in X)\n",
        "        self.coef_ = np.ones(X_train.shape[1])\n",
        "\n",
        "        # Loop through all epochs to update coefficients and intercept\n",
        "        for i in range(self.epochs):\n",
        "          for j in range(X_train.shape[0]):\n",
        "            idx = np.random.randint(0,X_train.shape[0])\n",
        "\n",
        "            y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_\n",
        "            intercept_der = -2 * (y_train[idx] - y_hat)\n",
        "            self.intercept_  = self.intercept_ - (self.lr * intercept_der)\n",
        "\n",
        "            coef_der = -2 * np.dot((y_train[idx] - y_hat) , X_train[idx])\n",
        "            self.coef_ = self.coef_ - (self.lr * coef_der)\n",
        "\n",
        "    # Predict method: used to predict outputs for test data\n",
        "    def predict(self, X_test):\n",
        "        # Prediction is the dot product of X and learned coefficients plus the intercept\n",
        "        return np.dot(X_test, self.coef_) + self.intercept_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gdr = SGDRegressor(epochs=100, learning_rate=0.01)"
      ],
      "metadata": {
        "id": "J-AxsuxbxokB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdr.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "LaaUbBmZ0Z-S"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = gdr.predict(X_test)"
      ],
      "metadata": {
        "id": "_RVqUrJQ29SF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u3un7h6U4KEo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}