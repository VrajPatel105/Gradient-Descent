{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "m0jC9tz1zQsd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = load_diabetes(return_X_y=True)"
      ],
      "metadata": {
        "id": "RPvMg8Ps3Js4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF82lMX63XAP",
        "outputId": "e6bcadad-58af-41d6-ab47-09b18eac82cb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(442, 10)\n",
            "(442,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 2)"
      ],
      "metadata": {
        "id": "A_h-5T-J3cI9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Gradient Descent\n",
        "Batch Gradient Descent is an optimization algorithm used to minimize the error (or loss) in machine learning models, particularly in regression problems. It works by updating the model parameters (like weights and bias) in the direction that reduces the loss function the most.\n",
        "\n",
        "\n",
        "Concept:\n",
        "In batch gradient descent, the algorithm uses the entire training dataset to compute the gradients before updating the parameters. This is different from stochastic and mini-batch gradient descent, which use smaller subsets of data."
      ],
      "metadata": {
        "id": "SNSAq5Xb4nWt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "a5AQ6CGShJ8J"
      },
      "outputs": [],
      "source": [
        "# Creating a custom class for Linear Regression using Gradient Descent\n",
        "class GDRegressor:\n",
        "\n",
        "    # Constructor to initialize learning rate and number of training epochs\n",
        "    def __init__(self, learning_rate=0.01, epochs=100):\n",
        "        self.coef_ = None        # This will hold the feature coefficients (weights)\n",
        "        self.intercept_ = None   # This will hold the bias/intercept\n",
        "        self.lr = learning_rate  # Learning rate for gradient descent\n",
        "        self.epochs = epochs     # Number of iterations for training\n",
        "\n",
        "    # Fit method: used to train the model on given data\n",
        "    def fit(self, X_train, y_train):\n",
        "        # Initialize the intercept to 0\n",
        "        self.intercept_ = 0\n",
        "\n",
        "        # Initialize coefficients to 1 for all features (same shape as number of columns in X)\n",
        "        self.coef_ = np.ones(X_train.shape[1])\n",
        "\n",
        "        # Loop through all epochs to update coefficients and intercept\n",
        "        for i in range(self.epochs):\n",
        "\n",
        "            # Predict the output using current weights and intercept\n",
        "            y_hat = np.dot(X_train, self.coef_) + self.intercept_\n",
        "\n",
        "            # Calculate derivative of loss with respect to intercept (mean of errors)\n",
        "            intercept_derivative = -2 * np.mean(y_train - y_hat)\n",
        "\n",
        "            # Update the intercept using gradient descent\n",
        "            self.intercept_ = self.intercept_ - (self.lr * intercept_derivative)\n",
        "\n",
        "            # Calculate derivative of loss with respect to each coefficient\n",
        "            coef_derivative = -2 * np.dot((y_train - y_hat), X_train) / X_train.shape[0]\n",
        "\n",
        "            # Update the coefficients using gradient descent\n",
        "            self.coef_ = self.coef_ - (self.lr * coef_derivative)\n",
        "\n",
        "    # Predict method: used to predict outputs for test data\n",
        "    def predict(self, X_test):\n",
        "        # Prediction is the dot product of X and learned coefficients plus the intercept\n",
        "        return np.dot(X_test, self.coef_) + self.intercept_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gdr = GDRegressor(epochs=100, learning_rate=0.01)"
      ],
      "metadata": {
        "id": "J-AxsuxbxokB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdr.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "LaaUbBmZ0Z-S"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = gdr.predict(X_test)"
      ],
      "metadata": {
        "id": "_RVqUrJQ29SF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u3un7h6U4KEo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}